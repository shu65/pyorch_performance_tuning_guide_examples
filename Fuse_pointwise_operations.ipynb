{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fuse pointwise operations.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOb88HhxHBkOD1Ul1q5AyPK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shu65/pyorch_performance_tuning_guide_examples/blob/main/Fuse_pointwise_operations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xpep5aKs-a5"
      },
      "source": [
        "\n",
        "PERFORMANCE TUNING GUIDE:\n",
        "\n",
        "https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html#fuse-pointwise-operations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BvGQqvT_TRAm",
        "outputId": "adb64e0b-edac-464e-c7dd-43ceb3ec1e2b"
      },
      "source": [
        "!pip list | grep torch"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch                         1.8.1+cu101   \n",
            "torchsummary                  1.5.1         \n",
            "torchtext                     0.9.1         \n",
            "torchvision                   0.9.1+cu101   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPRYz2oTTWw2",
        "outputId": "d56422cb-eb23-480b-cd3d-810ab9dfa5c9"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat May  8 11:11:26 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   50C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxmMExjVTaWe"
      },
      "source": [
        "import os\n",
        "import time\n",
        "\n",
        "import torch"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWdZSGv8TeKZ"
      },
      "source": [
        "def gelu(x):\n",
        "    return x * 0.5 * (1.0 + torch.erf(x / 1.41421))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxT9fhHJTgcO"
      },
      "source": [
        "input_batch_cpu = torch.randn(128, 3, 224, 224)\n",
        "input_batch_gpu = input_batch_cpu.clone().detach().to('cuda')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3Is-9LGZByx",
        "outputId": "b5da708c-83be-42ff-cb02-bbe86c1a2c47"
      },
      "source": [
        "# CPU default\n",
        "\n",
        "n_trials = 100\n",
        "out = gelu(input_batch_cpu)\n",
        "\n",
        "start = time.time()\n",
        "with torch.no_grad():\n",
        "  for i in range(n_trials):\n",
        "    out = gelu(input_batch_cpu)\n",
        "elapsed_time = time.time() - start\n",
        "\n",
        "print(\"avg cpu default:\", elapsed_time/n_trials, 'sec.')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "avg cpu default: 0.10857128143310547 sec.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOVJB59GZBoL",
        "outputId": "85a43023-1e7e-4adb-9f37-fe0335a13345"
      },
      "source": [
        "# CPU torch.jit.script\n",
        "torch.jit._state._jit_function_overload_caching.clear()\n",
        "torch.jit._state._jit_caching_layer.clear()\n",
        "\n",
        "n_trials = 100\n",
        "scripted_gelu = torch.jit.script(gelu)\n",
        "out = scripted_gelu(input_batch_cpu)\n",
        "\n",
        "start = time.time()\n",
        "with torch.no_grad():\n",
        "  for i in range(n_trials):\n",
        "    out = scripted_gelu(input_batch_cpu)\n",
        "elapsed_time = time.time() - start\n",
        "\n",
        "print(\"avg cpu torch.jit.script:\", elapsed_time/n_trials, 'sec.')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "avg cpu torch.jit.script: 0.11093469858169555 sec.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TIAu2CUlZk7u",
        "outputId": "139796bb-6069-4f66-c2b2-753bf7a45a9b"
      },
      "source": [
        "# GPU default\n",
        "\n",
        "n_trials = 1000\n",
        "out = gelu(input_batch_gpu)\n",
        "\n",
        "torch.cuda.synchronize()\n",
        "start = time.time()\n",
        "with torch.no_grad():\n",
        "  for i in range(n_trials):\n",
        "    out = gelu(input_batch_gpu)\n",
        "torch.cuda.synchronize()\n",
        "elapsed_time = time.time() - start\n",
        "\n",
        "print(\"avg gpu default:\", elapsed_time/n_trials, 'sec.')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "avg gpu default: 0.003557607650756836 sec.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TfmjW_3IpVCr",
        "outputId": "18b85142-1177-4140-f6a9-48291e8e4b5b"
      },
      "source": [
        "# GPU torch.jit.script\n",
        "torch.jit._state._jit_function_overload_caching.clear()\n",
        "torch.jit._state._jit_caching_layer.clear()\n",
        "\n",
        "n_trials = 1000\n",
        "scripted_gelu = torch.jit.script(gelu)\n",
        "out = scripted_gelu(input_batch_gpu)\n",
        "\n",
        "torch.cuda.synchronize()\n",
        "start = time.time()\n",
        "with torch.no_grad():\n",
        "  for i in range(n_trials):\n",
        "    out = scripted_gelu(input_batch_gpu)\n",
        "torch.cuda.synchronize()\n",
        "elapsed_time = time.time() - start\n",
        "\n",
        "print(\"avg gpu default:\", elapsed_time/n_trials, 'sec.')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "avg gpu default: 0.0007912750244140625 sec.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0oku0NfKqi_a",
        "outputId": "d2fa54a5-b9c8-48a9-ea2e-81e8f144924e"
      },
      "source": [
        "torch.jit._state._jit_function_overload_caching.clear()\n",
        "torch.jit._state._jit_caching_layer.clear()\n",
        "\n",
        "scripted_gelu = torch.jit.script(gelu)\n",
        "\n",
        "\n",
        "out = scripted_gelu(input_batch_cpu)\n",
        "print(\"1st graph \",torch.jit.last_executed_optimized_graph())\n",
        "out = scripted_gelu(input_batch_cpu)\n",
        "print(\"2nd graph \",torch.jit.last_executed_optimized_graph())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1st graph  graph(%x.1 : Tensor):\n",
            "  %1 : int = prim::Constant[value=1]()\n",
            "  %2 : float = prim::Constant[value=0.5]() # <ipython-input-4-5971bb91cfd0>:2:15\n",
            "  %3 : float = prim::Constant[value=1.]() # <ipython-input-4-5971bb91cfd0>:2:22\n",
            "  %4 : float = prim::Constant[value=1.41421]() # <ipython-input-4-5971bb91cfd0>:2:42\n",
            "  %10 : Tensor = prim::profile[profiled_type=Float(128, 3, 224, 224, strides=[150528, 50176, 224, 1], requires_grad=0, device=cpu)](%x.1)\n",
            "  %5 : Tensor = aten::mul(%10, %2) # <ipython-input-4-5971bb91cfd0>:2:11\n",
            "  %11 : Tensor = prim::profile[profiled_type=Float(128, 3, 224, 224, strides=[150528, 50176, 224, 1], requires_grad=0, device=cpu)](%x.1)\n",
            "  %6 : Tensor = aten::div(%11, %4) # <ipython-input-4-5971bb91cfd0>:2:38\n",
            "  %12 : Tensor = prim::profile[profiled_type=Float(128, 3, 224, 224, strides=[150528, 50176, 224, 1], requires_grad=0, device=cpu)](%6)\n",
            "  %7 : Tensor = aten::erf(%12) # <ipython-input-4-5971bb91cfd0>:2:28\n",
            "  %13 : Tensor = prim::profile[profiled_type=Float(128, 3, 224, 224, strides=[150528, 50176, 224, 1], requires_grad=0, device=cpu)](%7)\n",
            "  %8 : Tensor = aten::add(%13, %3, %1) # <string>:5:9\n",
            "  %14 : Tensor = prim::profile[profiled_type=Float(128, 3, 224, 224, strides=[150528, 50176, 224, 1], requires_grad=0, device=cpu)](%5)\n",
            "  %15 : Tensor = prim::profile[profiled_type=Float(128, 3, 224, 224, strides=[150528, 50176, 224, 1], requires_grad=0, device=cpu)](%8)\n",
            "  %9 : Tensor = aten::mul(%14, %15) # <ipython-input-4-5971bb91cfd0>:2:11\n",
            "  %16 : Tensor = prim::profile[profiled_type=Float(128, 3, 224, 224, strides=[150528, 50176, 224, 1], requires_grad=0, device=cpu)](%9)\n",
            "   = prim::profile()\n",
            "  return (%16)\n",
            "\n",
            "2nd graph  graph(%x.1 : Tensor):\n",
            "  %4 : float = prim::Constant[value=1.41421]() # <ipython-input-4-5971bb91cfd0>:2:42\n",
            "  %3 : float = prim::Constant[value=1.]() # <ipython-input-4-5971bb91cfd0>:2:22\n",
            "  %2 : float = prim::Constant[value=0.5]() # <ipython-input-4-5971bb91cfd0>:2:15\n",
            "  %1 : int = prim::Constant[value=1]()\n",
            "  %6 : Tensor = aten::mul(%x.1, %2) # <ipython-input-4-5971bb91cfd0>:2:11\n",
            "  %8 : Tensor = aten::div(%x.1, %4) # <ipython-input-4-5971bb91cfd0>:2:38\n",
            "  %10 : Tensor = aten::erf(%8) # <ipython-input-4-5971bb91cfd0>:2:28\n",
            "  %12 : Tensor = aten::add(%10, %3, %1) # <string>:5:9\n",
            "  %15 : Tensor = aten::mul(%6, %12) # <ipython-input-4-5971bb91cfd0>:2:11\n",
            "  return (%15)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_1JwiJ7ptD2",
        "outputId": "29cfd58a-c6d4-4bb9-fc83-b2fd17a4006b"
      },
      "source": [
        "torch.jit._state._jit_function_overload_caching.clear()\n",
        "torch.jit._state._jit_caching_layer.clear()\n",
        "\n",
        "scripted_gelu = torch.jit.script(gelu)\n",
        "\n",
        "\n",
        "out = scripted_gelu(input_batch_gpu)\n",
        "print(\"1st graph \",torch.jit.last_executed_optimized_graph())\n",
        "out = scripted_gelu(input_batch_gpu)\n",
        "print(\"2nd graph \",torch.jit.last_executed_optimized_graph())"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1st graph  graph(%x.1 : Tensor):\n",
            "  %1 : int = prim::Constant[value=1]()\n",
            "  %2 : float = prim::Constant[value=0.5]() # <ipython-input-4-5971bb91cfd0>:2:15\n",
            "  %3 : float = prim::Constant[value=1.]() # <ipython-input-4-5971bb91cfd0>:2:22\n",
            "  %4 : float = prim::Constant[value=1.41421]() # <ipython-input-4-5971bb91cfd0>:2:42\n",
            "  %10 : Tensor = prim::profile[profiled_type=Float(128, 3, 224, 224, strides=[150528, 50176, 224, 1], requires_grad=0, device=cuda:0)](%x.1)\n",
            "  %5 : Tensor = aten::mul(%10, %2) # <ipython-input-4-5971bb91cfd0>:2:11\n",
            "  %11 : Tensor = prim::profile[profiled_type=Float(128, 3, 224, 224, strides=[150528, 50176, 224, 1], requires_grad=0, device=cuda:0)](%x.1)\n",
            "  %6 : Tensor = aten::div(%11, %4) # <ipython-input-4-5971bb91cfd0>:2:38\n",
            "  %12 : Tensor = prim::profile[profiled_type=Float(128, 3, 224, 224, strides=[150528, 50176, 224, 1], requires_grad=0, device=cuda:0)](%6)\n",
            "  %7 : Tensor = aten::erf(%12) # <ipython-input-4-5971bb91cfd0>:2:28\n",
            "  %13 : Tensor = prim::profile[profiled_type=Float(128, 3, 224, 224, strides=[150528, 50176, 224, 1], requires_grad=0, device=cuda:0)](%7)\n",
            "  %8 : Tensor = aten::add(%13, %3, %1) # <string>:5:9\n",
            "  %14 : Tensor = prim::profile[profiled_type=Float(128, 3, 224, 224, strides=[150528, 50176, 224, 1], requires_grad=0, device=cuda:0)](%5)\n",
            "  %15 : Tensor = prim::profile[profiled_type=Float(128, 3, 224, 224, strides=[150528, 50176, 224, 1], requires_grad=0, device=cuda:0)](%8)\n",
            "  %9 : Tensor = aten::mul(%14, %15) # <ipython-input-4-5971bb91cfd0>:2:11\n",
            "  %16 : Tensor = prim::profile[profiled_type=Float(128, 3, 224, 224, strides=[150528, 50176, 224, 1], requires_grad=0, device=cuda:0)](%9)\n",
            "   = prim::profile()\n",
            "  return (%16)\n",
            "\n",
            "2nd graph  graph(%x.1 : Tensor):\n",
            "  %26 : Float(128, 3, 224, 224, strides=[150528, 50176, 224, 1], requires_grad=0, device=cuda:0), %27 : bool = prim::TypeCheck[types=[Float(128, 3, 224, 224, strides=[150528, 50176, 224, 1], requires_grad=0, device=cuda:0)]](%x.1)\n",
            "  %28 : Tensor = prim::If(%27)\n",
            "    block0():\n",
            "      %18 : Float(128, 3, 224, 224, strides=[150528, 50176, 224, 1], requires_grad=0, device=cuda:0) = prim::TensorExprGroup_0(%26)\n",
            "      -> (%18)\n",
            "    block1():\n",
            "      %39 : Function = prim::Constant[name=\"fallback_function\", fallback=1]()\n",
            "      %40 : (Tensor) = prim::CallFunction(%39, %x.1)\n",
            "      %41 : Tensor = prim::TupleUnpack(%40)\n",
            "      -> (%41)\n",
            "  return (%28)\n",
            "with prim::TensorExprGroup_0 = graph(%9 : Float(128, 3, 224, 224, strides=[150528, 50176, 224, 1], requires_grad=0, device=cuda:0)):\n",
            "  %5 : int = prim::Constant[value=1]()\n",
            "  %4 : float = prim::Constant[value=1.]()\n",
            "  %10 : float = prim::Constant[value=1.41421]()\n",
            "  %12 : float = prim::Constant[value=0.5]()\n",
            "  %13 : Float(128, 3, 224, 224, strides=[150528, 50176, 224, 1], requires_grad=0, device=cuda:0) = aten::mul(%9, %12) # <ipython-input-4-5971bb91cfd0>:2:11\n",
            "  %11 : Float(128, 3, 224, 224, strides=[150528, 50176, 224, 1], requires_grad=0, device=cuda:0) = aten::div(%9, %10) # <ipython-input-4-5971bb91cfd0>:2:38\n",
            "  %8 : Float(128, 3, 224, 224, strides=[150528, 50176, 224, 1], requires_grad=0, device=cuda:0) = aten::erf(%11) # <ipython-input-4-5971bb91cfd0>:2:28\n",
            "  %6 : Float(128, 3, 224, 224, strides=[150528, 50176, 224, 1], requires_grad=0, device=cuda:0) = aten::add(%8, %4, %5) # <string>:5:9\n",
            "  %2 : Float(128, 3, 224, 224, strides=[150528, 50176, 224, 1], requires_grad=0, device=cuda:0) = aten::mul(%13, %6) # <ipython-input-4-5971bb91cfd0>:2:11\n",
            "  return (%2)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jam49bb8sNkY",
        "outputId": "f395fa4c-fdb8-472c-99a3-d1806a61a248"
      },
      "source": [
        "# without optimized_execution\n",
        "\n",
        "torch.jit._state._jit_function_overload_caching.clear()\n",
        "torch.jit._state._jit_caching_layer.clear()\n",
        "\n",
        "with torch.jit.optimized_execution(False):\n",
        "    scripted_gelu = torch.jit.script(gelu)\n",
        "\n",
        "\n",
        "    out = scripted_gelu(input_batch_gpu)\n",
        "    print(\"1st graph \",torch.jit.last_executed_optimized_graph())\n",
        "    out = scripted_gelu(input_batch_gpu)\n",
        "    print(\"2nd graph \",torch.jit.last_executed_optimized_graph())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1st graph  graph(%x.1 : Tensor):\n",
            "  %1 : int = prim::Constant[value=1]()\n",
            "  %2 : float = prim::Constant[value=0.5]() # <ipython-input-4-5971bb91cfd0>:2:15\n",
            "  %3 : float = prim::Constant[value=1.]() # <ipython-input-4-5971bb91cfd0>:2:22\n",
            "  %4 : float = prim::Constant[value=1.41421]() # <ipython-input-4-5971bb91cfd0>:2:42\n",
            "  %5 : Tensor = aten::mul(%x.1, %2) # <ipython-input-4-5971bb91cfd0>:2:11\n",
            "  %6 : Tensor = aten::div(%x.1, %4) # <ipython-input-4-5971bb91cfd0>:2:38\n",
            "  %7 : Tensor = aten::erf(%6) # <ipython-input-4-5971bb91cfd0>:2:28\n",
            "  %8 : Tensor = aten::add(%7, %3, %1) # <string>:5:9\n",
            "  %9 : Tensor = aten::mul(%5, %8) # <ipython-input-4-5971bb91cfd0>:2:11\n",
            "  return (%9)\n",
            "\n",
            "2nd graph  graph(%x.1 : Tensor):\n",
            "  %1 : int = prim::Constant[value=1]()\n",
            "  %2 : float = prim::Constant[value=0.5]() # <ipython-input-4-5971bb91cfd0>:2:15\n",
            "  %3 : float = prim::Constant[value=1.]() # <ipython-input-4-5971bb91cfd0>:2:22\n",
            "  %4 : float = prim::Constant[value=1.41421]() # <ipython-input-4-5971bb91cfd0>:2:42\n",
            "  %5 : Tensor = aten::mul(%x.1, %2) # <ipython-input-4-5971bb91cfd0>:2:11\n",
            "  %6 : Tensor = aten::div(%x.1, %4) # <ipython-input-4-5971bb91cfd0>:2:38\n",
            "  %7 : Tensor = aten::erf(%6) # <ipython-input-4-5971bb91cfd0>:2:28\n",
            "  %8 : Tensor = aten::add(%7, %3, %1) # <string>:5:9\n",
            "  %9 : Tensor = aten::mul(%5, %8) # <ipython-input-4-5971bb91cfd0>:2:11\n",
            "  return (%9)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}